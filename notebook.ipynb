{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f41b0e-6500-4907-b0db-14649f02eff3",
   "metadata": {},
   "source": [
    "# Dogs Skin Disease Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "446f3ba8-82e8-4775-8b34-bd6ca6b555c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab96a0-60da-4ff0-8653-6e1c7fa3967c",
   "metadata": {},
   "source": [
    "base_dir = Path(\"Dataset\")\n",
    "IMG_HEIGHT = 244\n",
    "IMG_WIDTH = 244\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39718dac-a5dd-4e00-a99a-9e90b4f93e04",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d780f00-aef1-48bc-9ffb-1c1d13f30d91",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a4ab203-9ef4-4cf7-a810-5229db51c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT=244\n",
    "IMG_WIDTH=244\n",
    "BATCH_SIZE=8\n",
    "train_datagen = ImageDataGenerator(\n",
    "   rescale=1.0/255,\n",
    "   rotation_range=40,\n",
    "   width_shift_range=0.3,\n",
    "   height_shift_range=0.3,\n",
    "   shear_range=0.3,\n",
    "   zoom_range=0.3,\n",
    "   horizontal_flip=True,\n",
    "   vertical_flip=True,\n",
    "   brightness_range=[0.7,1.3],\n",
    "   channel_shift_range=50,\n",
    "   fill_mode='nearest',\n",
    "   validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe37573-a018-4de1-b8a3-47bab6d5f039",
   "metadata": {},
   "source": [
    "### Multiple Augmented Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77a6efad-d40a-407e-a406-87563de94003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_directory(source_dir, num_augmented=5):\n",
    "    for class_dir in source_dir.iterdir():\n",
    "        if class_dir.is_dir():\n",
    "            aug_dir = class_dir / 'augmented'\n",
    "            if aug_dir.exists():\n",
    "                continue  \n",
    "                \n",
    "            aug_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for img_path in class_dir.glob('*.[jp][pn][g]'):\n",
    "                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "                x = img_to_array(img)\n",
    "                x = x.reshape((1,) + x.shape)\n",
    "                \n",
    "                i = 0\n",
    "                for batch in train_datagen.flow(x, batch_size=1,\n",
    "                                             save_to_dir=aug_dir,\n",
    "                                             save_prefix=f'aug_{img_path.stem}',\n",
    "                                             save_format='jpg'):\n",
    "                    i += 1\n",
    "                    if i >= num_augmented:\n",
    "                        break\n",
    "                        \n",
    "base_dir = Path(\"Dataset\")  # or whatever your folder name is\n",
    "augment_directory(base_dir, num_augmented=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c9ce8-37b3-422d-9424-4be06948cb5c",
   "metadata": {},
   "source": [
    "### Creating generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f260430-bcc3-4b09-8e27-516ed1f38d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255  # Only rescaling since we're using pre-augmented images\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e1b6380-0b92-4e3f-98ad-24fc8e80a80d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 5 classes.\n",
      "Found 90 images belonging to 5 classes.\n",
      "Training samples (augmented): 900\n",
      "Validation samples (original): 90\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "# Create a separate directory for training and validation\n",
    "train_dir = base_dir / 'training_data'\n",
    "val_dir = base_dir / 'validation_data'\n",
    "\n",
    "if not train_dir.exists() or not val_dir.exists():\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "    val_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    class_names = ['Allergies', 'Autoimmune', 'Healthy', 'Infections', 'Parasites']\n",
    "    for class_name in class_names:\n",
    "        # Create directories\n",
    "        train_class_dir = train_dir / class_name\n",
    "        val_class_dir = val_dir / class_name\n",
    "        train_class_dir.mkdir(exist_ok=True)\n",
    "        val_class_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Copy augmented images to training directory\n",
    "        aug_dir = base_dir / class_name / 'augmented'\n",
    "        if aug_dir.exists():\n",
    "            for img_file in aug_dir.glob('*.[jp][pn][g]'):\n",
    "                shutil.copy2(img_file, train_class_dir)\n",
    "        \n",
    "        # Copy original images (excluding augmented folder) to validation directory\n",
    "        orig_dir = base_dir / class_name\n",
    "        for img_file in orig_dir.glob('*.[jp][pn][g]'):\n",
    "            if 'augmented' not in str(img_file):\n",
    "                shutil.copy2(img_file, val_class_dir)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "print(\"Training samples (augmented):\", train_generator.samples)\n",
    "print(\"Validation samples (original):\", validation_generator.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ea8ec7d-7b5b-4e3e-9067-5f5e4e11feb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: {'Allergies': 0, 'Autoimmune': 1, 'Healthy': 2, 'Infections': 3, 'Parasites': 4}\n",
      "Training samples: 900\n",
      "Validation samples: 90\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes:\", train_generator.class_indices)\n",
    "print(\"Training samples:\", train_generator.samples)\n",
    "print(\"Validation samples:\", validation_generator.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4352997-8461-411b-baad-961413d0420d",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68cb3961-94ee-4b28-aa35-01fe7c6fed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2, VGG16, EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f357135-3bb2-4d94-9712-e296e1a2be7b",
   "metadata": {},
   "source": [
    "### Getting all images and labels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8f5be-1064-47f5-ae0e-79a68e579ae8",
   "metadata": {},
   "source": [
    "As we have augmented images we need to collect the data from both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad5445cf-d55f-45e6-b1c2-f8377ce888b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples (augmented): 900\n",
      "Validation samples (original): 90\n"
     ]
    }
   ],
   "source": [
    "# Prepare separate lists for training (augmented) and validation (original) data\n",
    "train_data = []\n",
    "train_labels = []\n",
    "val_data = []\n",
    "val_labels = []\n",
    "\n",
    "# Define class names explicitly\n",
    "class_names = ['Allergies', 'Autoimmune', 'Healthy', 'Infections', 'Parasites']\n",
    "class_indices = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "# Collect data separately for training and validation\n",
    "for class_name in class_names:\n",
    "    class_dir = base_dir / class_name\n",
    "    if class_dir.is_dir():\n",
    "        class_index = class_indices[class_name]\n",
    "        \n",
    "        # Process original images for validation\n",
    "        for img_path in class_dir.glob('*.[jp][pn][g]'):\n",
    "            if 'augmented' not in str(img_path):  # Skip augmented directory\n",
    "                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "                x = img_to_array(img)\n",
    "                x = x / 255.0  # Normalize\n",
    "                val_data.append(x)\n",
    "                label = np.zeros(len(class_indices))\n",
    "                label[class_index] = 1\n",
    "                val_labels.append(label)\n",
    "        \n",
    "        # Process augmented images for training\n",
    "        aug_dir = class_dir / 'augmented'\n",
    "        if aug_dir.exists():\n",
    "            for img_path in aug_dir.glob('*.[jp][pn][g]'):\n",
    "                img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "                x = img_to_array(img)\n",
    "                x = x / 255.0  # Normalize\n",
    "                train_data.append(x)\n",
    "                label = np.zeros(len(class_indices))\n",
    "                label[class_index] = 1\n",
    "                train_labels.append(label)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "val_data = np.array(val_data)\n",
    "val_labels = np.array(val_labels)\n",
    "\n",
    "print(\"Training samples (augmented):\", len(train_data))\n",
    "print(\"Validation samples (original):\", len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f253ec-3e45-4bbd-9c1a-b8d7dae427bc",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32bafda6-5ef9-4dd0-b897-62057b48ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing model and training parameters\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_fold = None\n",
    "accuracies = []\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5af869e8-55fb-4e66-bd7e-8dbef57679fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ys/ndlktzd129z_51vvh7tqhc8m0000gn/T/ipykernel_18343/887229854.py:2: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n"
     ]
    }
   ],
   "source": [
    "#Creating model\n",
    "base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "                        include_top=False,\n",
    "                        weights='imagenet')\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f045307-8328-4f11-8d5f-a258730c97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "base_model = EfficientNetB0(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "                           include_top=False,\n",
    "                           weights='imagenet')\n",
    "\n",
    "# Fine-tune the last few layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:  # EfficientNet can benefit from fine-tuning more layers\n",
    "    layer.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(64, activation='relu',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,  # Smaller reduction factor\n",
    "    patience=4,\n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0003),  # Adjusted for EfficientNet\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,  # Increased patience\n",
    "    restore_best_weights=True,\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "# Add this when training\n",
    "callbacks = [early_stopping, lr_scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace06e26-b832-421f-9448-84a1a1b89801",
   "metadata": {},
   "source": [
    "### Training the model with k-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6070bf9-c874-4410-9fcc-fe4f83dfb665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1...\n",
      "Epoch 1/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3378 - loss: 1.7728 - val_accuracy: 0.6833 - val_loss: 0.8661\n",
      "Epoch 2/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 93ms/step - accuracy: 0.6919 - loss: 0.8308 - val_accuracy: 0.7667 - val_loss: 0.6339\n",
      "Epoch 3/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.8060 - loss: 0.5807 - val_accuracy: 0.8278 - val_loss: 0.4773\n",
      "Epoch 4/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step - accuracy: 0.8457 - loss: 0.4256 - val_accuracy: 0.8500 - val_loss: 0.4144\n",
      "Epoch 5/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.9108 - loss: 0.2865 - val_accuracy: 0.8389 - val_loss: 0.3902\n",
      "Epoch 6/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 87ms/step - accuracy: 0.8901 - loss: 0.2658 - val_accuracy: 0.8722 - val_loss: 0.3237\n",
      "Epoch 7/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 85ms/step - accuracy: 0.9328 - loss: 0.2243 - val_accuracy: 0.8778 - val_loss: 0.3205\n",
      "Epoch 8/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 84ms/step - accuracy: 0.9431 - loss: 0.1756 - val_accuracy: 0.9000 - val_loss: 0.2802\n",
      "Epoch 9/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 86ms/step - accuracy: 0.9673 - loss: 0.1252 - val_accuracy: 0.8611 - val_loss: 0.2952\n",
      "Epoch 10/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 102ms/step - accuracy: 0.9375 - loss: 0.1824 - val_accuracy: 0.9056 - val_loss: 0.2514\n",
      "Epoch 11/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 86ms/step - accuracy: 0.9631 - loss: 0.1260 - val_accuracy: 0.9167 - val_loss: 0.2239\n",
      "Epoch 12/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.9724 - loss: 0.0905 - val_accuracy: 0.9056 - val_loss: 0.2472\n",
      "Epoch 13/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9792 - loss: 0.0865 - val_accuracy: 0.9000 - val_loss: 0.2638\n",
      "Epoch 14/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - accuracy: 0.9497 - loss: 0.1292 - val_accuracy: 0.8722 - val_loss: 0.3659\n",
      "Epoch 15/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.9747 - loss: 0.0614 - val_accuracy: 0.8889 - val_loss: 0.3166\n",
      "Epoch 16/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.9652 - loss: 0.0807 - val_accuracy: 0.9167 - val_loss: 0.2511\n",
      "Fold 1 Accuracy: 0.9166666865348816\n",
      "Training Fold 2...\n",
      "Epoch 1/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 98ms/step - accuracy: 0.9308 - loss: 0.1806 - val_accuracy: 0.9889 - val_loss: 0.0671\n",
      "Epoch 2/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9466 - loss: 0.1825 - val_accuracy: 0.9944 - val_loss: 0.0516\n",
      "Epoch 3/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 95ms/step - accuracy: 0.9637 - loss: 0.1332 - val_accuracy: 0.9778 - val_loss: 0.0753\n",
      "Epoch 4/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9511 - loss: 0.1470 - val_accuracy: 0.9889 - val_loss: 0.0384\n",
      "Epoch 5/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9684 - loss: 0.1002 - val_accuracy: 0.9889 - val_loss: 0.0493\n",
      "Epoch 6/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 107ms/step - accuracy: 0.9807 - loss: 0.0676 - val_accuracy: 0.9889 - val_loss: 0.0486\n",
      "Epoch 7/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.9798 - loss: 0.0708 - val_accuracy: 0.9889 - val_loss: 0.0503\n",
      "Epoch 8/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 103ms/step - accuracy: 0.9805 - loss: 0.0662 - val_accuracy: 0.9833 - val_loss: 0.0426\n",
      "Epoch 9/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9781 - loss: 0.0584 - val_accuracy: 0.9944 - val_loss: 0.0380\n",
      "Epoch 10/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 96ms/step - accuracy: 0.9911 - loss: 0.0452 - val_accuracy: 0.9944 - val_loss: 0.0335\n",
      "Epoch 11/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9881 - loss: 0.0528 - val_accuracy: 0.9889 - val_loss: 0.0347\n",
      "Epoch 12/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 97ms/step - accuracy: 0.9887 - loss: 0.0446 - val_accuracy: 0.9722 - val_loss: 0.0583\n",
      "Epoch 13/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 151ms/step - accuracy: 0.9896 - loss: 0.0429 - val_accuracy: 0.9889 - val_loss: 0.0330\n",
      "Epoch 14/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 143ms/step - accuracy: 0.9934 - loss: 0.0261 - val_accuracy: 0.9889 - val_loss: 0.0350\n",
      "Epoch 15/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 131ms/step - accuracy: 0.9874 - loss: 0.0434 - val_accuracy: 0.9833 - val_loss: 0.0458\n",
      "Epoch 16/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 110ms/step - accuracy: 0.9893 - loss: 0.0349 - val_accuracy: 0.9944 - val_loss: 0.0409\n",
      "Epoch 17/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 112ms/step - accuracy: 0.9666 - loss: 0.1166 - val_accuracy: 0.9833 - val_loss: 0.0545\n",
      "Epoch 18/20\n",
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 125ms/step - accuracy: 0.9879 - loss: 0.0421 - val_accuracy: 0.9889 - val_loss: 0.0340\n"
     ]
    }
   ],
   "source": [
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_fold = None\n",
    "accuracies = []\n",
    "fold = 1\n",
    "\n",
    "# Training with cross-validation\n",
    "for train_index, val_index in kf.split(train_data):\n",
    "    print(f\"Training Fold {fold}...\")\n",
    "    x_train, x_val = train_data[train_index], train_data[val_index]\n",
    "    y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
    "    \n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=20,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    eval_result = model.evaluate(x_val, y_val, verbose=0)\n",
    "    accuracies.append(eval_result[1])\n",
    "    print(f\"Fold {fold} Accuracy: {eval_result[1]}\")\n",
    "    \n",
    "    if eval_result[1] > best_accuracy:\n",
    "        best_accuracy = eval_result[1]\n",
    "        best_fold = fold\n",
    "        best_model = tf.keras.models.clone_model(model)\n",
    "        best_model.set_weights(model.get_weights())\n",
    "    \n",
    "    fold += 1\n",
    "\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(accuracies)} ± {np.std(accuracies)}\")\n",
    "print(f\"Best Fold: {best_fold} with Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Final evaluation on validation data\n",
    "final_eval = best_model.evaluate(val_data, val_labels, verbose=0)\n",
    "print(f\"Final Validation Accuracy: {final_eval[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b4f18-2678-42ce-82ae-d16f63760719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cross-Validation Accuracy: {np.mean(accuracies)} ± {np.std(accuracies)}\")\n",
    "print(f\"Best Fold: {best_fold} with Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca5b0b-8a44-4da5-bd7f-aaac43843cfc",
   "metadata": {},
   "source": [
    "### Saving the best model in a keras file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f583b60-34f4-4b07-ac41-11b027db1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model is not None:\n",
    "    best_model.save('best_model.keras')\n",
    "    print(\"Best model saved as 'best_model.keras'\")\n",
    "\n",
    "with open('class_indices.json', 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f)\n",
    "    print(\"Class indices saved as 'class_indices.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b89cb2-e7d3-48dc-8206-62aed03ab7c2",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e13f01-80d6-48b4-9405-258488700c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "y_pred = best_model.predict(x_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_val, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "           xticklabels=train_generator.class_indices.keys(),\n",
    "           yticklabels=train_generator.class_indices.keys())\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ecbf0d-25a3-49cc-91ae-51a02eebe2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes, \n",
    "     target_names=train_generator.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38dbce3-263d-43fa-8aa3-41c963232009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
